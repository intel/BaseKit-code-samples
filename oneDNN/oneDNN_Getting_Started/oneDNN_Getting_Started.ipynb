{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.1 - Introduction to oneDNN - Getting Started\n",
    "\n",
    "## Learning Objectives\n",
    "In this module the developer will:\n",
    "* Learn different oneDNN releases inside the oneAPI toolkit\n",
    "* Learn how to compile a oneDNN sample with different releases via batch jobs on the Intel oneAPI DevCloud\n",
    "* Learn how to program oneDNN with a simple sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Getting Started Sample Exercise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## introduce oneDNN releases inside oneAPI toolkits\n",
    "oneDNN has four different releases inside the oneAPI toolkits. Each release is in a different folder under the oneDNN installation path, and each release supports a different compiler or threading library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the installation path of your oneAPI toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: ONEAPI_INSTALL=/home/intel/intel/inteloneapi/\n"
     ]
    }
   ],
   "source": [
    "%env ONEAPI_INSTALL=/home/intel/intel/inteloneapi/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/intel/intel/inteloneapi//oneDNN/latest/cpu_dpcpp_gpu_dpcpp\r\n",
      "/home/intel/intel/inteloneapi//oneDNN/latest/cpu_gomp\r\n",
      "/home/intel/intel/inteloneapi//oneDNN/latest/cpu_iomp\r\n",
      "/home/intel/intel/inteloneapi//oneDNN/latest/cpu_tbb\r\n"
     ]
    }
   ],
   "source": [
    "!printf '%s\\n'     $ONEAPI_INSTALL/oneDNN/latest/cpu_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are four different folders under the oneDNN installation path, and each of those releases supports different features. This tutorial will guide you how to compile and run against different oneDNN releases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a lab folder for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Editing the getting_started.cpp code\n",
    "The Jupyter cell below with the gray background can be edited in-place and saved.\n",
    "\n",
    "The first line of the cell contains the command **%%writefile 'lab/getting_started.cpp'** This tells the input cell to save the contents of the cell into the file name 'getting_started.cpp'  As you edit the cell and run it, it will save your changes into that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lab/getting_started.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile lab/getting_started.cpp\n",
    "\n",
    "/// This C++ API example demonstrates basics of DNNL programming\n",
    "/// model.\n",
    "\n",
    "\n",
    "#include <cmath>\n",
    "#include <numeric>\n",
    "#include <sstream>\n",
    "#include <vector>\n",
    "\n",
    "#include \"dnnl_debug.h\"\n",
    "#include \"example_utils.hpp\"\n",
    "\n",
    "using namespace dnnl;\n",
    "\n",
    "\n",
    "void getting_started_tutorial(engine::kind engine_kind) {\n",
    "\n",
    "    engine eng(engine_kind, 0);\n",
    "\n",
    "    stream engine_stream(eng);\n",
    "\n",
    "    const int N = 1, H = 13, W = 13, C = 3;\n",
    "\n",
    "    // Compute physical strides for each dimension\n",
    "    const int stride_N = H * W * C;\n",
    "    const int stride_H = W * C;\n",
    "    const int stride_W = C;\n",
    "    const int stride_C = 1;\n",
    "\n",
    "    // An auxiliary function that maps logical index to the physical offset\n",
    "    auto offset = [=](int n, int h, int w, int c) {\n",
    "        return n * stride_N + h * stride_H + w * stride_W + c * stride_C;\n",
    "    };\n",
    "\n",
    "    // The image size\n",
    "    const int image_size = N * H * W * C;\n",
    "\n",
    "    // Allocate a buffer for the image\n",
    "    std::vector<float> image(image_size);\n",
    "\n",
    "    // Initialize the image with some values\n",
    "    for (int n = 0; n < N; ++n)\n",
    "        for (int h = 0; h < H; ++h)\n",
    "            for (int w = 0; w < W; ++w)\n",
    "                for (int c = 0; c < C; ++c) {\n",
    "                    int off = offset(\n",
    "                            n, h, w, c); // Get the physical offset of a pixel\n",
    "                    image[off] = -std::cos(off / 10.f);\n",
    "                }\n",
    "\n",
    "    auto src_md = memory::desc(\n",
    "            {N, C, H, W}, // logical dims, the order is defined by a primitive\n",
    "            memory::data_type::f32, // tensor's data type\n",
    "            memory::format_tag::nhwc // memory format, NHWC in this case\n",
    "    );\n",
    "\n",
    "    auto alt_src_md = memory::desc(\n",
    "            {N, C, H, W}, // logical dims, the order is defined by a primitive\n",
    "            memory::data_type::f32, // tensor's data type\n",
    "            {stride_N, stride_C, stride_H, stride_W} // the strides\n",
    "    );\n",
    "\n",
    "    // Sanity check: the memory descriptors should be the same\n",
    "    if (src_md != alt_src_md)\n",
    "        throw std::string(\"memory descriptor initialization mismatch\");\n",
    "\n",
    "    auto src_mem = memory(src_md, eng);\n",
    "    write_to_dnnl_memory(image.data(), src_mem);\n",
    "\n",
    "    auto dst_mem = memory(src_md, eng);\n",
    "\n",
    "    auto relu_d = eltwise_forward::desc(\n",
    "            prop_kind::forward_inference, algorithm::eltwise_relu,\n",
    "            src_md, // the memory descriptor for an operation to work on\n",
    "            0.f, // alpha parameter means negative slope in case of ReLU\n",
    "            0.f // beta parameter is ignored in case of ReLU\n",
    "    );\n",
    "\n",
    "    // ReLU primitive descriptor, which corresponds to a particular\n",
    "    // implementation in the library\n",
    "    auto relu_pd\n",
    "            = eltwise_forward::primitive_desc(relu_d, // an operation descriptor\n",
    "                    eng // an engine the primitive will be created for\n",
    "            );\n",
    "\n",
    "\n",
    "    auto relu = eltwise_forward(relu_pd); // !!! this can take quite some time\n",
    "\n",
    "    relu.execute(engine_stream, // The execution stream\n",
    "            {\n",
    "                    // A map with all inputs and outputs\n",
    "                    {DNNL_ARG_SRC, src_mem}, // Source tag and memory obj\n",
    "                    {DNNL_ARG_DST, dst_mem}, // Destination tag and memory obj\n",
    "            });\n",
    "\n",
    "    // Wait the stream to complete the execution\n",
    "    engine_stream.wait();\n",
    "\n",
    "    std::vector<float> relu_image(image_size);\n",
    "    read_from_dnnl_memory(relu_image.data(), dst_mem);\n",
    "    \n",
    "    \n",
    "    // Check the results\n",
    "    for (int n = 0; n < N; ++n)\n",
    "        for (int h = 0; h < H; ++h)\n",
    "            for (int w = 0; w < W; ++w)\n",
    "                for (int c = 0; c < C; ++c) {\n",
    "                    int off = offset(\n",
    "                            n, h, w, c); // get the physical offset of a pixel\n",
    "                    float expected = image[off] < 0\n",
    "                            ? 0.f\n",
    "                            : image[off]; // expected value\n",
    "                    if (relu_image[off] != expected) {\n",
    "                        std::cout << \"At index(\" << n << \", \" << c << \", \" << h\n",
    "                                  << \", \" << w << \") expect \" << expected\n",
    "                                  << \" but got \" << relu_image[off]\n",
    "                                  << std::endl;\n",
    "                        throw std::logic_error(\"Accuracy check failed.\");\n",
    "                    }\n",
    "                }\n",
    "    // [Check the results]\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "    try {\n",
    "        engine::kind engine_kind = parse_engine_kind(argc, argv);\n",
    "        getting_started_tutorial(engine_kind);\n",
    "        std::cout << \"Example passes\" << std::endl;\n",
    "    } catch (dnnl::error &e) {\n",
    "        std::cerr << \"DNNL error: \" << e.what() << std::endl\n",
    "                  << \"Error status: \" << dnnl_status2str(e.status) << std::endl;\n",
    "        return 1;\n",
    "    } catch (std::string &e) {\n",
    "        std::cerr << \"Error in the example: \" << e << std::endl;\n",
    "        return 2;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "// [Main]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, copy the required header files and CMake file into lab folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $ONEAPI_INSTALL/oneDNN/latest/cpu_dpcpp_gpu_dpcpp/examples/example_utils.hpp lab/\n",
    "!cp $ONEAPI_INSTALL/oneDNN/latest/cpu_dpcpp_gpu_dpcpp/examples/example_utils.h lab/\n",
    "!cp $ONEAPI_INSTALL/oneDNN/latest/cpu_dpcpp_gpu_dpcpp/examples/CMakeLists.txt lab/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DevCloud Build and Execution\n",
    "This training is delivered and run on the Intel oneAPI DevCloud. To enable a large number of developers using the DevCloud simultaneously to enjoy the fullness of tools and access to a variety of hardware without delay, the DevCloud uses the Portable Batch System (PBS).\n",
    "\n",
    "As such, users must employ PBS utilities such as **qsub**, **pbsnodes**, **qstat**, and others to request and use compute resources. For more information, refer to [Using IntelÂ® DevCloud with oneAPI Product](https://software.intel.com/en-us/articles/using-intel-devcloud-with-oneapi-products).\n",
    "\n",
    "For training purposes we have written script utilities to ease developers in using the PBS system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove \"source setvars.sh\" from bash_profile for custom configuration of oneDNN\n",
    "In order to switch between different oneDNN releases, the user must use a custom configuration when running \"source setvars.sh\". However, bash_profile does \"source setvars.sh\" without any configuration by default. In order to switch between different releases successfully, we must remove the \"source setvars.sh\" in ~/.bash_profile by running the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%writefile ~/.bash_profile\n",
    "# .bash_profile\n",
    "\n",
    "# Get the aliases and functions\n",
    "if [ -f ~/.bashrc ]; then\n",
    "   . ~/.bashrc\n",
    "fi\n",
    "\n",
    "# User specific environment and startup programs\n",
    "export PATH=$PATH:$HOME/.local/bin:$HOME/bin\n",
    "\n",
    "# Enable Intel tools\n",
    "export INTEL_LICENSE_FILE=/usr/local/licenseserver/psxe.lic\n",
    "export PATH=/glob/intel-python/python3/bin/:/glob/intel-python/python2/bin/:${PATH}\n",
    "source /glob/development-tools/parallel-studio/bin/compilervars.sh intel64\n",
    "export PATH=$PATH:/bin\n",
    "#if [ -d /opt/intel/inteloneapi ]; then source /opt/intel/inteloneapi/setvars.sh > /dev/null 2>&1; fi\n",
    "\n",
    "# Make sure that most programs (in particular, pip) leave temp files locally\n",
    "if [ ! -d ${HOME}/tmp ]; then\n",
    "  mkdir ${HOME}/tmp\n",
    "fi\n",
    "export TMPDIR=${HOME}/tmp\n",
    "export ONEAPI_INSTALL=/opt/intel/inteloneapi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Run with DPC++ release \n",
    "one of the oneDNN releases supports DPC++, and it can run on different architectures by using DPC++.\n",
    "The following section guides you how to build with DPC++ and run on different architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - build.sh\n",
    "The script **build.sh** encapsulates the compiler **dpcpp** command and flags that will generate the exectuable.\n",
    "In order to use DPC++ compiler and related SYCL runtime, some definitions must be passed as cmake arguments.\n",
    "Here are related cmake arguments for DPC++ release : \n",
    "\n",
    "   -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=dpcpp -DDNNL_CPU_RUNTIME=SYCL -DDNNL_GPU_RUNTIME=SYCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing build.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile build.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --force> /dev/null 2>&1\n",
    "export EXAMPLE_ROOT=./lab/\n",
    "mkdir dpcpp\n",
    "cd dpcpp\n",
    "cmake .. -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=dpcpp -DDNNL_CPU_RUNTIME=SYCL -DDNNL_GPU_RUNTIME=SYCL\n",
    "make getting-started-cpp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you achieve an all-clear from your compilation, you execute your program on the DevCloud.\n",
    "\n",
    "#### Script - run.sh\n",
    "the script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "By default, the built program uses CPU as the execution engine, but the user can switch to GPU by giving an input argument \"gpu\".\n",
    "The user can refer run.sh below to run on GPU.\n",
    "To run on CPU, simply remove the input argument \"gpu\" ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --force > /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "./dpcpp/out/getting-started-cpp cpu\n",
    "echo \"########## Done with the run\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit the **build.sh** and **run.sh** to the job queue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The C compiler identification is Clang 10.0.0\n",
      "-- The CXX compiler identification is Clang 10.0.0\n",
      "-- Check for working C compiler: /home/intel/intel/inteloneapi/compiler/latest/linux/bin/clang\n",
      "-- Check for working C compiler: /home/intel/intel/inteloneapi/compiler/latest/linux/bin/clang -- works\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Check for working CXX compiler: /home/intel/intel/inteloneapi/compiler/latest/linux/bin/dpcpp\n",
      "-- Check for working CXX compiler: /home/intel/intel/inteloneapi/compiler/latest/linux/bin/dpcpp -- works\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- CMAKE_BUILD_TYPE is unset, defaulting to Release\n",
      "-- DNNLROOT: /home/intel/intel/inteloneapi/oneDNN/2021.1-beta04/cpu_dpcpp_gpu_dpcpp\n",
      "-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) \n",
      "-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) \n",
      "-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) \n",
      "-- Looking for CL_VERSION_2_2\n",
      "-- Looking for CL_VERSION_2_2 - found\n",
      "-- Found OpenCL: /home/intel/intel/inteloneapi/compiler/latest/linux/lib/libOpenCL.so (found version \"2.2\") \n",
      "-- Performing Test DPCPP_SUPPORTED\n",
      "-- Performing Test DPCPP_SUPPORTED - Success\n",
      "-- Found DPCPP: /home/intel/intel/inteloneapi/compiler/latest/linux/lib/libsycl.so  \n",
      "-- Found SYCL: /home/intel/intel/inteloneapi/compiler/latest/linux/lib/libsycl.so  \n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/intel/WORK/oneAPI/eco/DLDevKit-code-samples/oneDNN_Getting_Started/dpcpp\n",
      "\u001b[35m\u001b[1mScanning dependencies of target getting-started-cpp\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object out/CMakeFiles/getting-started-cpp.dir/getting_started.cpp.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable getting-started-cpp\u001b[0m\n",
      "[100%] Built target getting-started-cpp\n",
      "########## Executing the run\n",
      "Example passed on CPU.\n",
      "########## Done with the run\n"
     ]
    }
   ],
   "source": [
    "! rm -rf dpcpp;chmod 755 q; chmod 755 build.sh; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q build.sh; ./q run.sh; else ./build.sh; ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Build and Run with GNU Compiler with OpenMP release \n",
    "one of the oneDNN releases supports GNU compilers, but it can run only on CPU.\n",
    "The following section guides you how to build with G++ and run on CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - build.sh\n",
    "The script **build.sh** encapsulates the compiler command and flags that will generate the exectuable.\n",
    "The user must switch to the G++ oneDNN release by inputting a custom configuration \"--dnnl-configuration=cpu_gomp\" when running \"source setvars.sh\".\n",
    "In order to use the G++ compiler and related OMP runtime, some definitions must be passed as cmake arguments.\n",
    "Here are related cmake arguments for DPC++ release : \n",
    "\n",
    "  -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DDNNL_CPU_RUNTIME=OMP -DDNNL_GPU_RUNTIME=NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting build.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile build.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "export EXAMPLE_ROOT=./lab/\n",
    "mkdir cpu_gomp\n",
    "cd cpu_gomp\n",
    "cmake .. -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DDNNL_CPU_RUNTIME=OMP -DDNNL_GPU_RUNTIME=NONE\n",
    "make getting-started-cpp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you achieve an all-clear from your compilation, you execute your program on the DevCloud.\n",
    "\n",
    "#### Script - run.sh\n",
    "the script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "The user must switch to the G++ oneDNN release by inputting a custom configuration \"--dnnl-configuration=cpu_gomp\" when running \"source setvars.sh\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_gomp --force> /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "./cpu_gomp/out/getting-started-cpp\n",
    "echo \"########## Done with the run\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit the **build.sh** and **run.sh** to the job queue.\n",
    "\n",
    "##### NOTE - it is possible to execute any of the build and run commands in non-DevCloud environments (locally).\n",
    "To enable users to run their scripts both on the DevCloud and in local environments, this and subsequent training checks for the existence of the job submission command **qsub**.  If the check fails, it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The C compiler identification is GNU 7.4.0\n",
      "-- The CXX compiler identification is GNU 7.4.0\n",
      "-- Check for working C compiler: /usr/bin/gcc\n",
      "-- Check for working C compiler: /usr/bin/gcc -- works\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Check for working CXX compiler: /usr/bin/g++\n",
      "-- Check for working CXX compiler: /usr/bin/g++ -- works\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- CMAKE_BUILD_TYPE is unset, defaulting to Release\n",
      "-- DNNLROOT: /home/intel/intel/inteloneapi/oneDNN/2021.1-beta04/cpu_gomp\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/intel/WORK/oneAPI/eco/DLDevKit-code-samples/oneDNN_Getting_Started/cpu_gomp\n",
      "\u001b[35m\u001b[1mScanning dependencies of target getting-started-cpp\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object out/CMakeFiles/getting-started-cpp.dir/getting_started.cpp.o\u001b[0m\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable getting-started-cpp\u001b[0m\n",
      "[100%] Built target getting-started-cpp\n",
      "########## Executing the run\n",
      "Example passed on CPU.\n",
      "########## Done with the run\n"
     ]
    }
   ],
   "source": [
    "! rm -rf cpu_gomp;chmod 755 q; chmod 755 build.sh; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q build.sh; ./q run.sh; else ./build.sh; ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Build and Run with Intel Compiler with OpenMP release \n",
    "one of the oneDNN releases supports Intel compilers, but it can run only on CPU.\n",
    "The following section guides you how to build with ICC and run on CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - build.sh\n",
    "The script **build.sh** encapsulates the compiler command and flags that will generate the exectuable.\n",
    "The user must switch to the ICC oneDNN release by inputting a custom configuration \"--dnnl-configuration=cpu_iomp\" when running \"source setvars.sh\".\n",
    "In order to use ICC compiler and related OMP runtime, some definitions must be passed as cmake arguments.\n",
    "Here are related cmake arguments for DPC++ release : \n",
    "\n",
    "  -DCMAKE_C_COMPILER=icc -DCMAKE_CXX_COMPILER=icpc -DDNNL_CPU_RUNTIME=OMP -DDNNL_GPU_RUNTIME=NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_iomp --force> /dev/null 2>&1\n",
    "#source ~/intel/inteloneapi/setvars.sh  --dnnl-configuration=cpu_iomp > /dev/null 2>&1\n",
    "export EXAMPLE_ROOT=./lab/\n",
    "mkdir cpu_iomp\n",
    "cd cpu_iomp\n",
    "cmake .. -DCMAKE_C_COMPILER=icc -DCMAKE_CXX_COMPILER=icpc -DDNNL_CPU_RUNTIME=OMP -DDNNL_GPU_RUNTIME=NONE\n",
    "make getting-started-cpp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you achieve an all-clear from your compilation, you execute your program on the DevCloud.\n",
    "\n",
    "#### Script - run.sh\n",
    "the script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "The user must switch to the ICC oneDNN release by inputting a custom configuration \"--dnnl-configuration=cpu_iomp\" when running \"source setvars.sh\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_iomp > /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "./cpu_iomp/out/getting-started-cpp\n",
    "echo \"########## Done with the run\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit the **build.sh** and **run.sh** to the job queue.\n",
    "\n",
    "##### NOTE - it is possible to execute any of the build and run commands in non-DevCloud environments (locally).\n",
    "To enable users to run their scripts both on the DevCloud and in local environments, this and subsequent training checks for the existence of the job submission command **qsub**.  If the check fails it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 build.sh; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q build.sh; ./q run.sh; else ./build.sh; ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Build and Run with GNU Compiler with TBB release \n",
    "one of the oneDNN releases supports Intel Threading building block as its threading library, but it can run only on CPU.\n",
    "The following section guides you how to build with TBB and run on CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script - build.sh\n",
    "The script **build.sh** encapsulates the compiler **dpcpp** command and flags that will generate the exectuable.\n",
    "The user must switch to the G++ oneDNN release by inputting a custom configuration \"--dnnl-configuration=cpu_gomp\" when running \"source setvars.sh\".\n",
    "In order to use G++ compiler and related OMP runtime, some definitions must be passed as cmake arguments.\n",
    "Here are related cmake arguments for DPC++ release : \n",
    "\n",
    "  -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DDNNL_CPU_RUNTIME=OMP -DDNNL_GPU_RUNTIME=NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile build.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_tbb > /dev/null 2>&1\n",
    "export EXAMPLE_ROOT=./lab/\n",
    "mkdir cpu_tbb\n",
    "cd cpu_tbb\n",
    "cmake .. -DCMAKE_C_COMPILER=gcc -DCMAKE_CXX_COMPILER=g++ -DDNNL_CPU_RUNTIME=TBB -DDNNL_GPU_RUNTIME=NONE\n",
    "make getting-started-cpp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you achieve an all-clear from your compilation, you execute your program on the DevCloud.\n",
    "\n",
    "#### Script - run.sh\n",
    "the script **run.sh** encapsulates the program for submission to the job queue for execution.\n",
    "The user must switch to the TBBoneDNN release by inputting a custom configuration \"--dnnl-configuration=cpu_tbb\" when running \"source setvars.sh\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "source $ONEAPI_INSTALL/setvars.sh --dnnl-configuration=cpu_tbb --force> /dev/null 2>&1\n",
    "echo \"########## Executing the run\"\n",
    "./cpu_tbb/out/getting-started-cpp\n",
    "echo \"########## Done with the run\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Submitting **build.sh** and **run.sh** to the job queue\n",
    "Now we can submit the **build.sh** and **run.sh** to the job queue.\n",
    "\n",
    "##### NOTE - it is possible to execute any of the build and run commands in non-DevCloud environments (locally).\n",
    "To enable users to run their scripts both on the DevCloud and in local environments, this and subsequent training checks for the existence of the job submission command **qsub**.  If the check fails, it is assumed that build/run will be local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! chmod 755 q; chmod 755 build.sh; chmod 755 run.sh;if [ -x \"$(command -v qsub)\" ]; then ./q build.sh; ./q run.sh; else ./build.sh; ./run.sh; fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Summary\n",
    "In this lab the developer learned the following:\n",
    "* What are the different oneDNN releases inside the oneAPI toolkits\n",
    "* How to compile a oneDNN sample with different releases via batch jobs on the Intel oneAPI DevCloud\n",
    "* How to program oneDNN with a simple sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Transition Back To Slides"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "525.6px",
    "left": "28px",
    "top": "137.8px",
    "width": "301.109px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
